{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## __Protein Function Prediction Leveraging Sequence and Structure Information__\n",
        "\n",
        "> __Author:__ Grace Tang\n",
        "\n",
        "> __Edited:__ `04.23.25`\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "dHJe_PL6ooeo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### __Description__\n",
        "\n",
        "Predicting the functions of proteins is an essential problem in the study of proteins. Many previous works developed computational methods that utilize protein sequence information to predict protein functions. With the advancement of `AlphaFold2`, accurate protein structure data has been available for hundreds of millions of proteins.\n",
        "\n",
        "Incorporating structure information should be able to further boost the performance of protein function prediction methods. There are several potential ways to utilize the structure information: use computer-vision-like models to extract information from contact maps; use graph neural networks to encode 3D structures; use pretrained protein structure model (e.g., `ESM-IF`).\n",
        "\n",
        "In addition, with the advancement of LLMs, the text annotations of proteins can also be leveraged as an extra information source for protein function prediction, e.g., use LLM as encoder to encode the text annotations of proteins. Utilizing the information from sequence, structure, and text annotations, we can develop a model that accurately predicts protein functions.\n",
        "\n",
        "__Using a subset of the dataset used for [DeepFRI](https://github.com/flatironinstitute/DeepFRI) (a similar protein function prediction model) we aim to coallesce sequence data and structure data to predict protein function in the form of [EC Numbers](https://en.wikipedia.org/wiki/Enzyme_Commission_number).__\n",
        "\n",
        "[[1]](ttps://www.biorxiv.org/content/10.1101/2022.11.29.518451v1), [[2]](https://www.nature.com/articles/s42003-024-07359-z), [[3]](https://www.nature.com/articles/s41467-021-23303-9), [[4]](https://www.biorxiv.org/content/10.1101/2024.05.14.594226v1 )"
      ],
      "metadata": {
        "id": "KHXHtE130Hv8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### __Methodology__\n",
        "\n",
        "> _This section relies on having_ `annotations.tsv`, `train.txt`, `validation.txt`, _and_ `sequences.fasta` _in your runtime_\n"
      ],
      "metadata": {
        "id": "D9-0TiBuCrpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-geometric\n",
        "!pip install transformers biopython scipy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qC2NaAtxSLM3",
        "outputId": "3a5bc448-ea31-4669-e522-725c9bac4aeb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.19.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.1.31)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: biopython in /usr/local/lib/python3.11/dist-packages (1.85)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import requests\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.loader import DataLoader as GeometricDataLoader\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.optim as optim\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "from Bio.PDB import MMCIFParser\n",
        "\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "id": "0q7xGeHOFmAB"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start by defining some utility functions that help parse the input data into a representation that can be fed into our downstream `Dataset` classes."
      ],
      "metadata": {
        "id": "WSHjc-ikyz0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_fasta(path):\n",
        "    seqs = {}\n",
        "    with open(path) as f:\n",
        "        curr, lines = None, []\n",
        "        for line in f:\n",
        "            if line.startswith('>'):\n",
        "                if curr:\n",
        "                    seqs[curr] = ''.join(lines)\n",
        "                curr = line[1:].split()[0]\n",
        "                lines = []\n",
        "            else:\n",
        "                lines.append(line.strip())\n",
        "        if curr:\n",
        "            seqs[curr] = ''.join(lines)\n",
        "    return seqs\n",
        "\n",
        "def parse_annotations(path):\n",
        "    ann_map, all_ec = {}, set()\n",
        "    with open(path) as f:\n",
        "        for ln in f:\n",
        "            if ln.startswith('### PDB-chain'):\n",
        "                break\n",
        "        for ln in f:\n",
        "            if not ln.strip() or ln.startswith('#'): continue\n",
        "            pid, ecs = ln.strip().split('\\t')\n",
        "            ec_list = [e.strip() for e in ecs.split(',') if e.strip()]\n",
        "            ann_map[pid] = ec_list\n",
        "            all_ec.update(ec_list)\n",
        "    classes = sorted(all_ec)\n",
        "    return ann_map, classes\n",
        "\n",
        "def fetch_cif(pdb_chain, out_dir=\"structures\"):\n",
        "    pdb_id, chain = pdb_chain.split('-')\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    local_path = os.path.join(out_dir, f\"{pdb_chain}.cif\")\n",
        "    if not os.path.exists(local_path):\n",
        "        url = f\"https://files.rcsb.org/download/{pdb_id}.cif\"\n",
        "        resp = requests.get(url)\n",
        "        resp.raise_for_status()\n",
        "\n",
        "        with open(local_path, \"wb\") as f:\n",
        "            f.write(resp.content)\n",
        "    return local_path\n",
        "\n",
        "def build_df(ids, seqs, ann_map, classes):\n",
        "    rows = []\n",
        "    for pid in ids:\n",
        "        seq = seqs.get(pid, '')\n",
        "        if not seq:\n",
        "            print(f\"Warning: {pid} not found\")\n",
        "            continue\n",
        "        labels = [1 if ec in ann_map.get(pid, []) else 0 for ec in classes]\n",
        "        rows.append({'id': pid, 'sequence': seq, 'labels': labels})\n",
        "    return pd.DataFrame(rows)"
      ],
      "metadata": {
        "id": "C0wSTjxupV4o"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we read the `ids` from the `train.txt` and `validation.txt` to perform our _splitting_ into a train and validation dataset."
      ],
      "metadata": {
        "id": "6pl47LkPzVu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ids = [l.strip() for l in open('train.txt') if l.strip()]\n",
        "val_ids   = [l.strip() for l in open('validation.txt') if l.strip()]\n",
        "seqs_map  = parse_fasta('sequences.fasta')\n",
        "ann_map, classes = parse_annotations('annotations.tsv')"
      ],
      "metadata": {
        "id": "1ZcJ1IpoGs_3"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This `Dataset` class feeds each protein through two parallel pipelines — sequence and structure — and returns a single `torch_geometric.data.Data` object per example for graph‐based training.\n",
        "\n",
        "1. **Structure Fetching:** Given an ID like `4PR3-A`, it downloads the corresponding `mmCIF` file from `RCSB` if not already cached. Uses Biopython’s `PDBParser` to extract all Cα atom coordinates for the specified chain.\n",
        "\n",
        "2. **Graph Construction:** Builds an undirected residue‐level graph by connecting any two Cα atoms within 8 Å.  Encodes edge weights as the inverse distance (`1/d`) to reflect spatial proximity.\n",
        "\n",
        "3. **Node Features:** Represents each residue by a 20‐dimensional one-hot vector (one position per standard amino acid). Ensures that non-standard letters (U, Z, O, B) are mapped to “X” and treated uniformly.\n",
        "\n",
        "4. **`ProtBert` Embedding** Cleans and “space-separates” the amino-acid string for `Rostlab/prot_bert`.  Tokenizes, pads/truncates, and runs the model in `eval()` mode to get the last hidden states.  Applies mean-pooling (masking out padding) to produce a fixed‐size (1024-dim) global embedding.\n",
        "\n",
        "5. **Label Vector:** Builds a multi-hot target vector for all EC classes based on `annotations.tsv`.\n",
        "\n",
        "6. **Output**  \n",
        "   - Returns a `Data` object with fields:  \n",
        "     - `x`: `[L, 20]` residue features  \n",
        "     - `edge_index`: `[2, E]` graph connectivity  \n",
        "     - `edge_attr`: `[E, 1]` distance weights  \n",
        "     - `seq_emb`: `[1024]` `ProtBert` fingerprint  \n",
        "     - `y`: `[num_ec]` multi-hot EC labels  \n"
      ],
      "metadata": {
        "id": "2lvFNxNH2Y7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ProteinDataset(Dataset):\n",
        "    def __init__(self, ids, seqs_map, ann_map, classes,\n",
        "                 max_length=512, device=None, ca_cutoff=8.0,\n",
        "                 pdb_dir=\"structures\"):\n",
        "        self.ids      = ids\n",
        "        self.seqs_map = seqs_map\n",
        "        self.labels   = {\n",
        "            pid: torch.tensor([1 if ec in ann_map.get(pid, []) else 0\n",
        "                               for ec in classes], dtype=torch.float)\n",
        "            for pid in ids\n",
        "        }\n",
        "        self.classes = classes\n",
        "        self.max_length = max_length\n",
        "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.ca_cutoff = ca_cutoff\n",
        "        self.pdb_dir = pdb_dir\n",
        "\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
        "        self.bert      = BertModel.from_pretrained(\"Rostlab/prot_bert\").to(self.device).eval()\n",
        "\n",
        "        self.parser = MMCIFParser(QUIET=True)\n",
        "\n",
        "        aa_list = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
        "        self.aa2idx = {aa:i for i, aa in enumerate(aa_list)}\n",
        "        self.num_tokens = len(aa_list)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pid = self.ids[idx]\n",
        "\n",
        "        seq = self.seqs_map[pid].upper()\n",
        "        clean = re.sub(r\"[^ACDEFGHIKLMNPQRSTVWY]\", \"X\", seq)\n",
        "        spaced = \" \".join(clean)\n",
        "        enc = self.tokenizer(\n",
        "            spaced, padding=\"max_length\", truncation=True,\n",
        "            max_length=self.max_length, return_tensors=\"pt\"\n",
        "        )\n",
        "        input_ids      = enc[\"input_ids\"].to(self.device)\n",
        "        attention_mask = enc[\"attention_mask\"].to(self.device)\n",
        "        with torch.no_grad():\n",
        "            out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            last = out.last_hidden_state                  # [1, L, H]\n",
        "            mask = attention_mask.unsqueeze(-1).float()   # [1, L, 1]\n",
        "            summed = (last * mask).sum(1)                 # [1, H]\n",
        "            lengths = mask.sum(1)\n",
        "            seq_emb = (summed / lengths).squeeze(0).cpu() # [H]\n",
        "\n",
        "        cif_path = fetch_cif(pid, out_dir=self.pdb_dir)\n",
        "        struct = self.parser.get_structure(pid, cif_path)\n",
        "        ca_coords = []\n",
        "        chain_id = pid.split(\"-\")[1]\n",
        "        for model in struct:\n",
        "            for chain in model:\n",
        "                if chain.id == chain_id:\n",
        "                    for res in chain:\n",
        "                        if \"CA\" in res:\n",
        "                            ca_coords.append(res[\"CA\"].get_coord())\n",
        "        coords = np.array(ca_coords)\n",
        "\n",
        "        dmat = squareform(pdist(coords))\n",
        "        row, col = np.where((dmat < self.ca_cutoff) & (dmat > 0))\n",
        "        edge_index = torch.tensor([row, col], dtype=torch.long)\n",
        "        edge_attr  = torch.tensor((1.0 / dmat[row, col])[:, None], dtype=torch.float)\n",
        "\n",
        "        feats = []\n",
        "        for aa in clean:\n",
        "            vec = np.zeros(self.num_tokens, dtype=float)\n",
        "            if aa in self.aa2idx:\n",
        "                vec[self.aa2idx[aa]] = 1.0\n",
        "            feats.append(vec)\n",
        "        x = torch.tensor(feats, dtype=torch.float)\n",
        "\n",
        "        return Data(\n",
        "            x=x,\n",
        "            edge_index=edge_index,\n",
        "            edge_attr=edge_attr,\n",
        "            seq_emb=seq_emb,\n",
        "            y=self.labels[pid]\n",
        "        )"
      ],
      "metadata": {
        "id": "GRcX0Gi2HFid"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using our `Dataset` class now we can input our training and validation data and define some `DataLoaders` to be used by our chosen model.\n",
        "\n",
        "> We are leveraging a `df_train_mini` subset for faster development for our final model we will use the _full_ training set."
      ],
      "metadata": {
        "id": "7JnaYElK6Yzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_mini = train_ids[:1000]\n",
        "\n",
        "train_ds = ProteinDataset(df_train_mini, seqs_map, ann_map, classes, pdb_dir='structures')\n",
        "val_ds   = ProteinDataset(val_ids,       seqs_map, ann_map, classes, pdb_dir='structures')\n",
        "\n",
        "train_loader = GeometricDataLoader(train_ds, batch_size=16, shuffle=True)\n",
        "val_loader   = GeometricDataLoader(val_ds,   batch_size=16)"
      ],
      "metadata": {
        "id": "euSOVKLjHQEe"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This `ConvClassifer` ingests the `Data` objects produced by `ProteinDataset` and fuses structure and sequence information to predict one or more EC numbers.\n",
        "\n",
        "1. **Structure Branch (GCN)**  \n",
        "   - **GCNConv(20 → 128)** + ReLU  \n",
        "   - **GCNConv(128 → 128)** + ReLU  \n",
        "   - **Global Mean Pooling** over residues → a 128-dim graph embedding\n",
        "\n",
        "2. **Sequence Branch (MLP)**  \n",
        "   - **Linear(1024 → 128)** + ReLU  \n",
        "   - Takes the `ProtBert` pooled embedding and projects it down to 128 dims\n",
        "\n",
        "3. **Fusion**  \n",
        "   - **Concatenate** the 128-dim GCN output and 128-dim MLP output → 256-dim vector  \n",
        "   - **MLP Head**: Linear(256 → 256) + ReLU + Dropout(0.1) + Linear(256 → `num_ec`)  \n",
        "   - Outputs one raw logit per EC class\n",
        "\n",
        "4. **Training Details**  \n",
        "   - **Loss**: `BCEWithLogitsLoss` for multi-label classification  \n",
        "   - **Optimizer**: AdamW with `lr=1e-3`, `weight_decay=1e-5`  \n",
        "   - **DataLoader**: uses `torch_geometric.loader.DataLoader` to batch graphs  "
      ],
      "metadata": {
        "id": "t0sGdR8L904y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvClassifier(nn.Module):\n",
        "    def __init__(self, seq_emb_dim, node_feat_dim, gcn_hidden, num_classes):\n",
        "        super().__init__()\n",
        "        self.seq_mlp = nn.Sequential(\n",
        "            nn.Linear(seq_emb_dim, gcn_hidden),  # was backward before\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.conv1 = GCNConv(node_feat_dim, gcn_hidden)\n",
        "        self.conv2 = GCNConv(gcn_hidden,      gcn_hidden)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(2 * gcn_hidden, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, edge_attr, batch = (\n",
        "            data.x, data.edge_index, data.edge_attr, data.batch\n",
        "        )\n",
        "        h = self.conv1(x, edge_index, edge_weight=edge_attr.squeeze())\n",
        "        h = torch.relu(h)\n",
        "        h = self.conv2(h, edge_index, edge_weight=edge_attr.squeeze())\n",
        "        h = torch.relu(h)\n",
        "        gcn_emb = global_mean_pool(h, batch)  # → [B, gcn_hidden]\n",
        "\n",
        "        seq_emb_raw = data.seq_emb             # [B, seq_emb_dim]\n",
        "        seq_emb = seq_emb_raw.to(gcn_emb.device)\n",
        "        seq_emb = self.seq_mlp(seq_emb)        # → [B, gcn_hidden]\n",
        "\n",
        "        comb = torch.cat([gcn_emb, seq_emb], dim=1)  # → [B, 256]\n",
        "        return self.classifier(comb)\n"
      ],
      "metadata": {
        "id": "uVuQfxW5HVwh"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example   = train_ds[0]\n",
        "seq_dim   = example.seq_emb.shape[0]\n",
        "node_dim  = example.x.shape[1]\n",
        "\n",
        "model = ConvClassifier(seq_emb_dim=seq_dim,\n",
        "                      node_feat_dim=node_dim,\n",
        "                      gcn_hidden=128,\n",
        "                      num_classes=len(classes)).to(torch.device('cuda'))\n",
        "opt   = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "crit  = nn.BCEWithLogitsLoss()"
      ],
      "metadata": {
        "id": "PAbyGQ3aHX91"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, 11):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(model.classifier[0].weight.device)\n",
        "        logits = model(batch)\n",
        "        loss = crit(logits, batch.y)\n",
        "        opt.zero_grad(); loss.backward(); opt.step()\n",
        "        total_loss += loss.item() * batch.num_graphs\n",
        "    print(f\"Epoch {epoch} Train Loss: {total_loss/len(train_ds):.4f}\")\n",
        "\n",
        "    model.eval()\n",
        "    all_preds, all_targs = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            batch = batch.to(model.classifier[0].weight.device)\n",
        "            probs = torch.sigmoid(model(batch)).cpu()\n",
        "            all_preds.append(probs)\n",
        "            all_targs.append(batch.y.cpu())\n",
        "    preds = torch.vstack(all_preds).numpy() > 0.5\n",
        "    targs = torch.vstack(all_targs).numpy() > 0.5\n",
        "    print(\"Val P, R, F1=\",\n",
        "          precision_score(targs, preds, average='micro', zero_division=0),\n",
        "          recall_score(targs, preds, average='micro', zero_division=0),\n",
        "          f1_score(targs, preds, average='micro', zero_division=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "e6if8lNHHe4n",
        "outputId": "542432a2-8eb3-4932-f13f-c04cb948b8bc"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (1x16384 and 1024x128)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-94d512153585>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-52-036344247b4d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mseq_emb_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_emb\u001b[0m             \u001b[0;31m# [B, seq_emb_dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mseq_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq_emb_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgcn_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mseq_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_mlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_emb\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# → [B, gcn_hidden]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# --- DEBUGGING (uncomment to inspect) ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x16384 and 1024x128)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "all_true, all_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for seqs, labs in val_loader:\n",
        "        seqs = seqs.to(device)\n",
        "        logits = model(seqs)\n",
        "        probs  = torch.sigmoid(logits).cpu()\n",
        "        all_true.append(labs)\n",
        "        all_pred.append((probs > 0.5).int())\n",
        "\n",
        "true = torch.cat(all_true, dim=0)\n",
        "pred = torch.cat(all_pred, dim=0)\n",
        "\n",
        "rows = []\n",
        "for pid, trow, prow in zip(val_ids, true, pred):\n",
        "    true_ecs = [c for c, flag in zip(classes, trow) if flag]\n",
        "    pred_ecs = [c for c, flag in zip(classes, prow) if flag]\n",
        "    rows.append({\n",
        "        'strain': pid,\n",
        "        'validation EC class': ';'.join(true_ecs) or '-',\n",
        "        'predicted': ';'.join(pred_ecs) or '-'\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(rows, columns=['strain', 'validation EC class', 'predicted'])\n",
        "print(results_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6w6LYQwTV6Z",
        "outputId": "21a9497c-5630-494b-b4a1-44243398ecf7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        strain validation EC class predicted\n",
            "0       1EF9-A             4.1.1.-         -\n",
            "1       4BYF-A             3.6.4.-         -\n",
            "2       1MVP-A            3.4.23.-         -\n",
            "3       2BIH-A             1.7.1.-         -\n",
            "4     6UE0-AAA     4.3.3.-;4.3.3.7         -\n",
            "...        ...                 ...       ...\n",
            "1724    2F9R-A             4.6.1.-   3.2.1.-\n",
            "1725    1QQW-A   1.11.1.-;1.11.1.6         -\n",
            "1726    6AHR-E   3.1.26.-;3.1.26.5         -\n",
            "1727    2BJI-A    3.1.3.-;3.1.3.25         -\n",
            "1728    3BAL-A           1.13.11.-         -\n",
            "\n",
            "[1729 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8Fue1sTRyxn8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yifan-grace-tang/final-project/blob/main/Renee/renee's_tweaks_grace_mlcb_04_29.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## __Protein Function Prediction Leveraging Sequence and Structure Information__\n",
        "\n",
        "> __Author:__ Grace Tang\n",
        "\n",
        "> __Edited:__ `04.29.25`\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "dHJe_PL6ooeo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### __Description__"
      ],
      "metadata": {
        "id": "KHXHtE130Hv8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predicting the functions of proteins is an essential problem in the study of proteins. Many previous works developed computational methods that utilize protein sequence information to predict protein functions. With the advancement of `AlphaFold2`, accurate protein structure data has been available for hundreds of millions of proteins.\n",
        "\n",
        "Incorporating structure information should be able to further boost the performance of protein function prediction methods. There are several potential ways to utilize the structure information: use computer-vision-like models to extract information from contact maps; use graph neural networks to encode 3D structures; use pretrained protein structure model (e.g., `ESM-IF`).\n",
        "\n",
        "In addition, with the advancement of LLMs, the text annotations of proteins can also be leveraged as an extra information source for protein function prediction, e.g., use LLM as encoder to encode the text annotations of proteins. Utilizing the information from sequence, structure, and text annotations, we can develop a model that accurately predicts protein functions.\n",
        "\n",
        "__Using a subset of the dataset used for [DeepFRI](https://github.com/flatironinstitute/DeepFRI) (a similar protein function prediction model) we aim to coallesce sequence data and structure data to predict protein function in the form of [EC Numbers](https://en.wikipedia.org/wiki/Enzyme_Commission_number).__\n",
        "\n",
        "[[1]](ttps://www.biorxiv.org/content/10.1101/2022.11.29.518451v1), [[2]](https://www.nature.com/articles/s42003-024-07359-z), [[3]](https://www.nature.com/articles/s41467-021-23303-9), [[4]](https://www.biorxiv.org/content/10.1101/2024.05.14.594226v1 )"
      ],
      "metadata": {
        "id": "RoCPxSvgisj5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### __Pre-Requisites__"
      ],
      "metadata": {
        "id": "77uCDyHbhWIQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While not _strictly_ neccessary - you can import your [Hugging Face](https://huggingface.co/settings/tokens/new?tokenType=read) token into your colab's secrets as `HF_TOKEN` to supress some annoying warnings.\n",
        "\n",
        "If you would like to clear cached embeddings under `cache/` run the cell below. Note, this __shouldn't be neccessary__ unless the `ProteinDataset` feature construction changes. Under no circumstances would it be neccessary to remove the `structures/` as pulling the data from online takes quite a bit of time. __Use the next cell wisely!__"
      ],
      "metadata": {
        "id": "vaucx-FTiY5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -rf structures/*\n",
        "# !rm -rf cache/*"
      ],
      "metadata": {
        "id": "FbZsxzmFikxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-geometric &> /dev/null\n",
        "!pip install transformers biopython scipy &> /dev/null"
      ],
      "metadata": {
        "id": "cOUMf8Vzhacl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "id": "jHf6DwVcfRDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import requests\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.loader import DataLoader as GeometricDataLoader\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.optim as optim\n",
        "from torch.serialization import add_safe_globals\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.data.data import DataEdgeAttr, DataTensorAttr\n",
        "from torch_geometric.data.storage import GlobalStorage\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from Bio.PDB import MMCIFParser\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "add_safe_globals([Data, DataEdgeAttr, DataTensorAttr, GlobalStorage])"
      ],
      "metadata": {
        "id": "RDj1zdLIhdJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### __Data Cleaning__"
      ],
      "metadata": {
        "id": "ttSjv9zxfejv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "> _This section relies on having_ `annotations.tsv`, `train.txt`, `validation.txt`, _and_ `sequences.fasta` _in your runtime_\n"
      ],
      "metadata": {
        "id": "t5naSVZGixcq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start by defining some utility functions that help parse the input data into a representation that can be fed into our downstream `Dataset` classes."
      ],
      "metadata": {
        "id": "fzY258y7hoa2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_fasta(path):\n",
        "    seqs = {}\n",
        "    with open(path) as f:\n",
        "        curr, lines = None, []\n",
        "        for line in f:\n",
        "            if line.startswith('>'):\n",
        "                if curr:\n",
        "                    seqs[curr] = ''.join(lines)\n",
        "                curr = line[1:].split()[0]\n",
        "                lines = []\n",
        "            else:\n",
        "                lines.append(line.strip())\n",
        "        if curr:\n",
        "            seqs[curr] = ''.join(lines)\n",
        "    return seqs\n",
        "\n",
        "def parse_annotations(path):\n",
        "    ann_map, all_ec = {}, set()\n",
        "    with open(path) as f:\n",
        "        for ln in f:\n",
        "            if ln.startswith('### PDB-chain'):\n",
        "                break\n",
        "        for ln in f:\n",
        "            if not ln.strip() or ln.startswith('#'): continue\n",
        "            pid, ecs = ln.strip().split('\\t')\n",
        "            ec_list = [e.strip() for e in ecs.split(',') if e.strip()]\n",
        "            ann_map[pid] = ec_list\n",
        "            all_ec.update(ec_list)\n",
        "    classes = sorted(all_ec)\n",
        "    return ann_map, classes\n",
        "\n",
        "def fetch_cif(pdb_chain, out_dir=\"structures\"):\n",
        "    pdb_id, chain = pdb_chain.split('-')\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    local_path = os.path.join(out_dir, f\"{pdb_chain}.cif\")\n",
        "    if not os.path.exists(local_path):\n",
        "        url = f\"https://files.rcsb.org/download/{pdb_id}.cif\"\n",
        "        resp = requests.get(url)\n",
        "        resp.raise_for_status()\n",
        "\n",
        "        with open(local_path, \"wb\") as f:\n",
        "            f.write(resp.content)\n",
        "    return local_path\n",
        "\n",
        "def build_df(ids, seqs, ann_map, classes):\n",
        "    rows = []\n",
        "    for pid in ids:\n",
        "        seq = seqs.get(pid, '')\n",
        "        if not seq:\n",
        "            print(f\"Warning: {pid} not found in fasta\")\n",
        "            continue\n",
        "\n",
        "        ecs = ann_map.get(pid)\n",
        "        if ecs is None or len(ecs)!=1:\n",
        "            continue\n",
        "\n",
        "        ec = ecs[0]\n",
        "        if ec not in classes:\n",
        "            continue\n",
        "\n",
        "        label_idx = classes.index(ec)\n",
        "        rows.append({\n",
        "            'id'      : pid,\n",
        "            'sequence': seq,\n",
        "            'label'   : label_idx\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(rows)"
      ],
      "metadata": {
        "id": "2cI1ScRmhp67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predicting multiple `EC` labels for a particular protein is a _hard_ problem, therefore for model performance while maintaining utility we have refined the problem space down to building a model that can predict the first three \"`EC`-snippets\" for a particular protein.\n",
        "\n",
        "Additionally, to avoid multi-class classification we remove any training/validation data points that contain multiple three-level `EC` classifications. Below demonstrates the process:"
      ],
      "metadata": {
        "id": "TZr0CQ0pmx99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ann_map, _ = parse_annotations('annotations.tsv')\n",
        "\n",
        "filtered_map = {}\n",
        "\n",
        "for pid, labels in ann_map.items():\n",
        "  filtered = { \".\".join(ec.split(\".\")[:3]) for ec in labels }\n",
        "  if len(filtered) == 1:\n",
        "    filtered_map[pid] = [ filtered.pop() ]\n",
        "\n",
        "print(f\"Original: {len(ann_map)}\")\n",
        "print(f\"Filtered: {len(filtered_map)}\")"
      ],
      "metadata": {
        "id": "hYoDuUhNnSIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we read the `ids` from the `train.txt` and `validation.txt` to perform our _splitting_ into a train and validation dataset with slight modifications for the trimmed `EC` classes:\n",
        "- `TRAINING_DATA_SIZE` defines how many datapoints from `train.txt` are used for model training.\n",
        "- `VALIDATION_RATIO` defines how many datapoints from `validation.txt` are used for model evaluation (as a fraction of `TRAINING_DATA_SIZE`)\n"
      ],
      "metadata": {
        "id": "6pl47LkPzVu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAINING_DATA_SIZE = 2000\n",
        "VALIDATION_RATIO   = 0.2\n",
        "\n",
        "VALIDATION_DATA_SIZE = int(TRAINING_DATA_SIZE * VALIDATION_RATIO)\n",
        "\n",
        "train_ids = [l.strip() for l in open('train.txt') if l.strip() if l.strip() in filtered_map][:TRAINING_DATA_SIZE]\n",
        "val_ids   = [l.strip() for l in open('validation.txt') if l.strip() if l.strip() in filtered_map][:VALIDATION_DATA_SIZE]\n",
        "\n",
        "classes = sorted(ecs[0] for ecs in filtered_map.values())\n",
        "\n",
        "seqs_map = parse_fasta('sequences.fasta')\n",
        "\n",
        "df_train = build_df(train_ids, seqs_map, filtered_map, classes)\n",
        "df_val   = build_df(val_ids, seqs_map, filtered_map, classes)"
      ],
      "metadata": {
        "id": "1ZcJ1IpoGs_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### __The `ProteinDataset`__"
      ],
      "metadata": {
        "id": "SqCfPcoYvOPz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This `Dataset` class feeds each protein through two parallel pipelines — sequence and structure — and returns a single `torch_geometric.data.Data` object per example for graph‐based training.\n",
        "\n",
        "1. **Structure Fetching:** Given an ID like `4PR3-A`, it downloads the corresponding `mmCIF` file from `RCSB` if not already cached. Uses Biopython’s `PDBParser` to extract all Cα atom coordinates for the specified chain.\n",
        "\n",
        "2. **Graph Construction:** Builds an undirected residue‐level graph by connecting any two Cα atoms within 8 Å.  Encodes edge weights as the inverse distance (`1/d`) to reflect spatial proximity.\n",
        "\n",
        "3. **Node Features:** Represents each residue by a 20‐dimensional one-hot vector (one position per standard amino acid). Ensures that non-standard letters (U, Z, O, B) are mapped to “X” and treated uniformly.\n",
        "\n",
        "4. **`ProtBert` Embedding** Cleans and “space-separates” the amino-acid string for `Rostlab/prot_bert`.  Tokenizes, pads/truncates, and runs the model in `eval()` mode to get the last hidden states.  Applies mean-pooling (masking out padding) to produce a fixed‐size (1024-dim) global embedding.\n",
        "\n",
        "5. **Label Vector:** Builds a multi-hot target vector for all EC classes based on `annotations.tsv`.\n",
        "\n",
        "6. **Output**  \n",
        "   - Returns a `Data` object with fields:  \n",
        "     - `x`: `[L, 20]` residue features  \n",
        "     - `edge_index`: `[2, E]` graph connectivity  \n",
        "     - `edge_attr`: `[E, 1]` distance weights  \n",
        "     - `seq_emb`: `[1024]` `ProtBert` fingerprint  \n",
        "     - `y`: `[num_ec]` multi-hot EC labels  \n"
      ],
      "metadata": {
        "id": "2lvFNxNH2Y7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ProteinDataset(Dataset):\n",
        "    AA       = \"ACDEFGHIKLMNPQRSTVWY\"\n",
        "    AA2IDX   = {a:i for i,a in enumerate(AA)}\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        df,\n",
        "        max_length=512,\n",
        "        device=None,\n",
        "        ca_cutoff=8.0,\n",
        "        pdb_dir=\"structures\",\n",
        "        bert_model=\"Rostlab/prot_bert\",\n",
        "    ):\n",
        "        self.df         = df.reset_index(drop=True)\n",
        "        self.max_length = max_length\n",
        "        self.device     = device or torch.device(\"cpu\")\n",
        "        self.ca_cutoff  = ca_cutoff\n",
        "        self.pdb_dir    = pdb_dir\n",
        "        os.makedirs(pdb_dir, exist_ok=True)\n",
        "\n",
        "        self.parser    = MMCIFParser(QUIET=True)\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(bert_model, do_lower_case=False)\n",
        "        self.bert      = BertModel.from_pretrained(bert_model).eval().to(self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        cache_path = f\"cache/{idx}.pt\"\n",
        "\n",
        "        if os.path.exists(cache_path):\n",
        "            return torch.load(cache_path)\n",
        "\n",
        "        row    = self.df.iloc[idx]\n",
        "        pid    = row[\"id\"]\n",
        "        seq    = row[\"sequence\"]\n",
        "        label  = torch.tensor(int(row[\"label\"]), dtype=torch.long)\n",
        "\n",
        "        cif_path  = fetch_cif(pid, out_dir=self.pdb_dir)\n",
        "        structure = self.parser.get_structure(pid, cif_path)\n",
        "\n",
        "        coords = [\n",
        "            (int(res.get_id()[1]), atom.get_coord())\n",
        "            for model in structure\n",
        "            for chain in model\n",
        "            for res in chain\n",
        "            for atom in res\n",
        "            if atom.get_id() == \"CA\"\n",
        "        ]\n",
        "        coords.sort(key=lambda x: x[0])\n",
        "\n",
        "        clean = re.sub(r\"[^ACDEFGHIKLMNPQRSTVWY]\", \"X\", seq.upper())\n",
        "        feats = []\n",
        "        for aa in clean:\n",
        "            vec = torch.zeros(len(self.AA))\n",
        "            vec[self.AA2IDX.get(aa, 0)] = 1.0\n",
        "            feats.append(vec)\n",
        "        x = torch.stack(feats, dim=0)\n",
        "\n",
        "        positions = [resi-1 for resi, _ in coords]\n",
        "\n",
        "        ca = torch.tensor([c for _, c in coords], dtype=torch.float)\n",
        "        D  = torch.tensor(squareform(pdist(ca.numpy())), dtype=torch.float)\n",
        "\n",
        "        row, col = (D <= self.ca_cutoff).nonzero().t()\n",
        "\n",
        "        edge_index = torch.tensor([\n",
        "            [positions[r] for r in row],\n",
        "            [positions[c] for c in col]\n",
        "        ], dtype=torch.long)\n",
        "\n",
        "        edge_attr = (1.0 / (D[row, col] + 1e-8)).unsqueeze(1)\n",
        "\n",
        "        tokens = self.tokenizer(\n",
        "            \" \".join(clean),\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length\n",
        "        ).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = self.bert(**tokens).last_hidden_state\n",
        "\n",
        "        mask    = tokens[\"attention_mask\"].unsqueeze(-1)\n",
        "        seq_emb = (out * mask).sum(1) / mask.sum(1)\n",
        "        seq_emb = seq_emb.squeeze(0).cpu()\n",
        "\n",
        "        data = Data(\n",
        "            x          = x,\n",
        "            edge_index = edge_index,\n",
        "            edge_attr  = edge_attr,\n",
        "            seq_emb    = seq_emb,\n",
        "            y          = label\n",
        "        )\n",
        "\n",
        "        os.makedirs(\"cache\", exist_ok=True)\n",
        "        torch.save(data, cache_path)\n",
        "        return data\n"
      ],
      "metadata": {
        "id": "GRcX0Gi2HFid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using our `Dataset` class now we can input our training and validation data and define some `DataLoaders` to be used by our chosen model."
      ],
      "metadata": {
        "id": "7JnaYElK6Yzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = ProteinDataset(df_train,  device=torch.device('cuda'))\n",
        "val_ds   = ProteinDataset(df_val,    device=torch.device('cuda'))\n",
        "\n",
        "train_loader = GeometricDataLoader(train_ds, batch_size=16, shuffle=True)\n",
        "val_loader   = GeometricDataLoader(val_ds,   batch_size=16)"
      ],
      "metadata": {
        "id": "euSOVKLjHQEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### __The `ConvClassifier`__"
      ],
      "metadata": {
        "id": "vrcmPiH_xwNy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This `ConvClassifer` ingests the `Data` objects produced by `ProteinDataset` and fuses structure and sequence information to predict an EC number.\n",
        "\n",
        "1. **Structure Branch (GCN)**  \n",
        "   - **`GCNConv(20 → 128)`** + `ReLU`\n",
        "   - **`GCNConv(128 → 128)`** + `ReLU`  \n",
        "   - **Global Mean Pooling** over residues → a `128`-dim graph embedding\n",
        "\n",
        "2. **Sequence Branch (MLP)**  \n",
        "   - **`Linear(1024 → 128)`** + `ReLU`  \n",
        "   - Takes the `ProtBert` pooled embedding and projects it down to `128` dims\n",
        "\n",
        "3. **Fusion**  \n",
        "   - **Concatenate** the `128`-dim GCN output and `128`-dim MLP output → `256`-dim vector  \n",
        "   - **MLP Head**: `Linear(256 → 256)` + `ReLU` + `Dropout(0.1)` + `Linear(256 → num_ec)`  \n"
      ],
      "metadata": {
        "id": "t0sGdR8L904y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvClassifier(nn.Module):\n",
        "    def __init__(self, seq_emb_dim, node_feat_dim, gcn_hidden, num_classes):\n",
        "        super().__init__()\n",
        "        self.seq_mlp = nn.Sequential(\n",
        "            nn.Linear(seq_emb_dim, gcn_hidden),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.conv1 = GCNConv(node_feat_dim, gcn_hidden)\n",
        "        self.conv2 = GCNConv(gcn_hidden,      gcn_hidden)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(2 * gcn_hidden, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, edge_attr, batch = (\n",
        "            data.x, data.edge_index, data.edge_attr, data.batch\n",
        "        )\n",
        "\n",
        "        edge_index = edge_index.clamp(0, x.size(0) - 1)\n",
        "\n",
        "        h = self.conv1(x, edge_index, edge_weight=edge_attr.squeeze())\n",
        "        h = torch.relu(h)\n",
        "        h = self.conv2(h, edge_index, edge_weight=edge_attr.squeeze())\n",
        "        h = torch.relu(h)\n",
        "        gcn_emb = global_mean_pool(h, batch)\n",
        "\n",
        "        seq_emb_raw = data.seq_emb\n",
        "        B = gcn_emb.size(0)\n",
        "        seq_emb = seq_emb_raw.view(B, -1).to(gcn_emb.device)\n",
        "\n",
        "        seq_emb = self.seq_mlp(seq_emb)\n",
        "\n",
        "        comb = torch.cat([gcn_emb, seq_emb], dim=1)\n",
        "        return self.classifier(comb)\n"
      ],
      "metadata": {
        "id": "uVuQfxW5HVwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample    = train_ds[0]\n",
        "seq_dim   = sample.seq_emb.shape[0]\n",
        "node_dim  = sample.x.shape[1]\n",
        "\n",
        "model = ConvClassifier(seq_emb_dim=seq_dim,\n",
        "                      node_feat_dim=node_dim,\n",
        "                      gcn_hidden=128,\n",
        "                      num_classes=len(classes)).to(torch.device('cuda'))"
      ],
      "metadata": {
        "id": "PAbyGQ3aHX91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### __Model Training__"
      ],
      "metadata": {
        "id": "fTCoa1Jg5wYI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We train the classifier for `10` epochs. In each epoch the model alternates between:\n",
        "\n",
        "1. **Training Phase**  \n",
        "   - Switch to `model.train()`  \n",
        "   - Iterate over `train_loader`, compute **CrossEntropyLoss** between logits and integer class labels  \n",
        "   - Backpropagate with **AdamW** and accumulate the total loss  \n",
        "   - Report **average training loss** at epoch end\n",
        "\n",
        "2. **Validation Phase**  \n",
        "   - Switch to `model.eval()` and disable gradients  \n",
        "   - Iterate over `val_loader`, run a forward pass and take `argmax` over logits  \n",
        "   - Collect predictions and targets, then compute **Accuracy**, **Precision**, **Recall** and **F₁** (micro)\n"
      ],
      "metadata": {
        "id": "QUvVflDl7BZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 50\n",
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "opt   = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
        "crit  = nn.CrossEntropyLoss()\n",
        "\n",
        "header = f\"| {'Epoch':>5s} | {'Train Loss':>10s} | {'Val Acc':>8s} | {'Prec':>6s} | {'Rec':>6s} | {'F1':>6s} |\"\n",
        "print(header)\n",
        "print('-' * len(header))\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(device)\n",
        "        logits = model(batch)\n",
        "        labels = batch.y\n",
        "        loss   = crit(logits, labels)\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        total_loss += loss.item() * batch.num_graphs\n",
        "    train_loss = total_loss / len(train_ds)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    model.eval()\n",
        "    all_preds, all_targs = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            batch  = batch.to(device)\n",
        "            logits = model(batch).cpu()\n",
        "            preds  = logits.argmax(dim=1)\n",
        "            targs  = batch.y.cpu()\n",
        "            all_preds.append(preds)\n",
        "            all_targs.append(targs)\n",
        "            val_loss += loss.item() * batch.num_graphs\n",
        "\n",
        "    val_loss /= len(val_ds)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    preds = torch.cat(all_preds).numpy()\n",
        "    targs = torch.cat(all_targs).numpy()\n",
        "\n",
        "    val_acc = (preds == targs).mean()\n",
        "    p       = precision_score(targs, preds, average='macro', zero_division=0)\n",
        "    r       = recall_score (targs, preds, average='macro', zero_division=0)\n",
        "    f1      = f1_score     (targs, preds, average='macro', zero_division=0)\n",
        "\n",
        "    print(\n",
        "        f\"{epoch:5d} | \"\n",
        "        f\"{train_loss:10.4f} | \"\n",
        "        f\"{val_acc*100:7.2f}% | \"\n",
        "        f\"{p*100:6.2f}% | \"\n",
        "        f\"{r*100:6.2f}% | \"\n",
        "        f\"{f1*100:6.2f}%\"\n",
        "    )"
      ],
      "metadata": {
        "id": "e6if8lNHHe4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### __Model Validation__"
      ],
      "metadata": {
        "id": "cXR09AZ2zbWy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We are visualizing which EC numbers the model will have a higher tendency to confuse:**"
      ],
      "metadata": {
        "id": "2Iq7eIGXzeno"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting Loss over Epochs to demonstrate how well the model is learning:"
      ],
      "metadata": {
        "id": "F6sLD3_f3AjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_losses, label=\"Train Loss\", linewidth=2, marker='o')\n",
        "plt.plot(val_losses, label=\"Validation Loss\", linewidth=2, marker='s')\n",
        "\n",
        "plt.title(\"Loss over Epochs\", fontsize=16)\n",
        "plt.xlabel(\"Epoch\", fontsize=14)\n",
        "plt.ylabel(\"Loss\", fontsize=14)\n",
        "plt.legend(fontsize=12)\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uOmBpzN59Zaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plot below visualizes the model's confidence in its prediction across all of the validation proteins.\n",
        "\n",
        "Black bars indicate correct predictions and red bars indicate incorrect ones:"
      ],
      "metadata": {
        "id": "SAxIVXpG4SWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Patch\n",
        "\n",
        "all_examples = sorted(all_examples, key=lambda x: x['conf'], reverse=True)\n",
        "confidences = [ex['conf'] for ex in all_examples]\n",
        "colors = ['black' if ex['true'] == ex['pred'] else 'red' for ex in all_examples]\n",
        "\n",
        "plt.figure(figsize=(22, 6))\n",
        "plt.bar(range(len(all_examples)), confidences, color=colors)\n",
        "\n",
        "xtick_positions = list(range(0, len(all_examples), 20))\n",
        "plt.xticks(xtick_positions)\n",
        "\n",
        "legend_elements = [\n",
        "    Patch(facecolor='black', label='Correct Prediction'),\n",
        "    Patch(facecolor='red', label='Incorrect Prediction')\n",
        "]\n",
        "plt.legend(handles=legend_elements, loc='upper right', fontsize=10)\n",
        "\n",
        "plt.ylim(0, 1.1)\n",
        "plt.title(\"Model Confidence on Validation Set (Sorted by Confidence)\", fontsize=14)\n",
        "plt.ylabel(\"Confidence Score (Softmax)\", fontsize=12)\n",
        "plt.xlabel(\"Validation Proteins (sorted)\", fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "J49OwAZHv8fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Printing total incorrect predictions below:"
      ],
      "metadata": {
        "id": "2JC3Cev-79RF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wrong_preds = [ex for ex in examples if ex[\"true\"] != ex[\"pred\"]]\n",
        "print(f\"Incorrect Predictions: {len(wrong_preds)} / {len(examples)}\")\n"
      ],
      "metadata": {
        "id": "XwXRaPz5JfRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ex in wrong_preds:\n",
        "    print(f\"True: {ex['true']} | Pred: {ex['pred']} | Conf: {ex['conf']:.2f}\")\n"
      ],
      "metadata": {
        "id": "hqqKjOIvKEdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also plotted how confident the model was in wrong predictions, below is a plot, sorted by most to least confident wrong predictions:\n"
      ],
      "metadata": {
        "id": "M-zCU-037fVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.patches import Patch\n",
        "\n",
        "wrong_sorted = sorted(wrong_preds, key=lambda x: x['conf'], reverse=True)\n",
        "confidences = [ex['conf'] for ex in wrong_sorted]\n",
        "labels = [f\"T:{ex['true']}\\nP:{ex['pred']}\" for ex in wrong_sorted]\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "colors = sns.color_palette(\"Reds\", len(wrong_sorted))\n",
        "bars = plt.bar(range(len(wrong_sorted)), confidences, color=colors)\n",
        "\n",
        "for i, ex in enumerate(wrong_sorted):\n",
        "    plt.text(i, ex['conf'] + 0.015, labels[i], ha='center', fontsize=8, rotation=90)\n",
        "\n",
        "legend_elements = [Patch(color='red', label='Misclassified Protein'),\n",
        "                   Patch(facecolor='white', edgecolor='black', label='T: True Class\\nP: Predicted Class')]\n",
        "plt.legend(handles=legend_elements, loc='upper right', fontsize=10)\n",
        "\n",
        "plt.ylim(0, 1.1)\n",
        "plt.title(\"Confidence of All Incorrect Predictions (Sorted)\", fontsize=14)\n",
        "plt.ylabel(\"Confidence (Softmax)\", fontsize=12)\n",
        "plt.xlabel(\"Misclassified Proteins\", fontsize=12)\n",
        "plt.xticks([])\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "yZ7de1ct6NZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### __Cleanup__"
      ],
      "metadata": {
        "id": "6pggIutuRxjJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For faster processing in the future we download the `structures/` and `cache/` directories as `zip` archives that can be unzipped in a development environment for faster future environment.\n",
        "\n",
        "Note, that any changes to the `ProteinDataset` class will require a deletion of the `cache/` whereas the `structures/` cache should __never__ have to be recomputed."
      ],
      "metadata": {
        "id": "6y_uk6DlR4tJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r structures.zip structures &> /dev/null\n",
        "!zip -r cache.zip cache &> /dev/null"
      ],
      "metadata": {
        "id": "L9BigIoYSJqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!torch.save(model.state_dict(), \"final_model.pt\")\n"
      ],
      "metadata": {
        "id": "GVUASnnCWER9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!model.load_state_dict(torch.load(\"final_model.pt\"))\n",
        "!model.eval()\n"
      ],
      "metadata": {
        "id": "BiXDiUWdWHHf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}